{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-05T21:07:31.151568Z",
     "start_time": "2025-04-05T21:07:31.148567Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T21:07:31.176737Z",
     "start_time": "2025-04-05T21:07:31.168726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Load the data\n",
    "# Assume the dataset is saved as 'airline_delay.csv'\n",
    "def load_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(\"\\nSample data:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nData types:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"\\nMissing values:\")\n",
    "    print(df.isnull().sum())\n",
    "    print(\"\\nClass distribution:\")\n",
    "    print(df['Class'].value_counts())\n",
    "    return df\n",
    "\n",
    "def preprocess_data(df):\n",
    "    data = df.copy()\n",
    "\n",
    "    for col in data.columns:\n",
    "        if data[col].isnull().sum() > 0:\n",
    "            if data[col].dtype == 'object':\n",
    "                data[col].fillna(data[col].mode()[0], inplace=True)\n",
    "            else:\n",
    "                data[col].fillna(data[col].median(), inplace=True)\n",
    "\n",
    "    numeric_features = ['Time', 'Length'] # removed Flight\n",
    "    categorical_features = ['Airline', 'AirportFrom', 'AirportTo', 'DayOfWeek']\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', SimpleImputer(strategy='median'), numeric_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "        ])\n",
    "\n",
    "    X = data.drop('Class', axis=1)\n",
    "    y = data['Class']\n",
    "\n",
    "    return X, y, preprocessor, numeric_features, categorical_features\n",
    "\n",
    "# 3. Model evaluation with cross-validation\n",
    "def evaluate_model(model, X, y, preprocessor, cv=5, balance=False):\n",
    "    # Create full pipeline with preprocessing\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "    # Initialize StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "    # For storing metrics\n",
    "    y_true_all = []\n",
    "    y_pred_all = []\n",
    "    y_prob_all = []\n",
    "\n",
    "    # Cross-validation loop\n",
    "    for train_idx, test_idx in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        # Apply class balancing if requested\n",
    "        if balance:\n",
    "            smote = SMOTE(random_state=42)\n",
    "            X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "            X_train_preprocessed, y_train = smote.fit_resample(X_train_preprocessed, y_train)\n",
    "\n",
    "            # Fit on balanced data\n",
    "            pipeline.steps[0] = ('preprocessor', 'passthrough')  # Skip preprocessing as it's already done\n",
    "            pipeline.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "            # For prediction, we need to preprocess test data\n",
    "            X_test_preprocessed = preprocessor.transform(X_test)\n",
    "            y_pred = pipeline.predict(X_test_preprocessed)\n",
    "            y_prob = pipeline.predict_proba(X_test_preprocessed)[:, 1]\n",
    "        else:\n",
    "            # Regular fit without balancing\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "            y_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Collect predictions\n",
    "        y_true_all.extend(y_test)\n",
    "        y_pred_all.extend(y_pred)\n",
    "        y_prob_all.extend(y_prob)\n",
    "\n",
    "    # Calculate metrics\n",
    "    conf_matrix = confusion_matrix(y_true_all, y_pred_all)\n",
    "    accuracy = accuracy_score(y_true_all, y_pred_all)\n",
    "    precision = precision_score(y_true_all, y_pred_all, average=None)\n",
    "    recall = recall_score(y_true_all, y_pred_all, average=None)\n",
    "    f1 = f1_score(y_true_all, y_pred_all, average=None)\n",
    "    f1_global = f1_score(y_true_all, y_pred_all, average='weighted')\n",
    "\n",
    "    # Calculate AUC if possible\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true_all, y_prob_all)\n",
    "        fpr, tpr, _ = roc_curve(y_true_all, y_prob_all)\n",
    "    except:\n",
    "        auc = None\n",
    "        fpr, tpr = None, None\n",
    "\n",
    "    results = {\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'f1_global': f1_global,\n",
    "        'auc': auc,\n",
    "        'roc_curve': (fpr, tpr),\n",
    "        'y_true': y_true_all,\n",
    "        'y_pred': y_pred_all,\n",
    "        'y_prob': y_prob_all\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "def report_results(model_name, results):\n",
    "    print(f\"\\n=== {model_name} Results ===\")\n",
    "    print(f\"Confusion Matrix:\\n{results['confusion_matrix']}\")\n",
    "    print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
    "\n",
    "    # For binary classification\n",
    "    print(f\"Precision - Class 0: {results['precision'][0]:.4f}, Class 1: {results['precision'][1]:.4f}\")\n",
    "    print(f\"Recall - Class 0: {results['recall'][0]:.4f}, Class 1: {results['recall'][1]:.4f}\")\n",
    "    print(f\"F1 Score - Class 0: {results['f1_score'][0]:.4f}, Class 1: {results['f1_score'][1]:.4f}\")\n",
    "    print(f\"Global F1 Score: {results['f1_global']:.4f}\")\n",
    "\n",
    "    if results['auc'] is not None:\n",
    "        print(f\"AUC: {results['auc']:.4f}\")\n",
    "\n",
    "    # Plot ROC curve if available\n",
    "    if results['roc_curve'][0] is not None:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(results['roc_curve'][0], results['roc_curve'][1],\n",
    "                 lw=2, label=f'{model_name} (AUC = {results[\"auc\"]:.4f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curve - {model_name}')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "\n",
    "    return"
   ],
   "id": "29f56fef553348fb",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T21:07:31.182742Z",
     "start_time": "2025-04-05T21:07:31.178742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def optimize_decision_tree(X, y, preprocessor, cv=5):\n",
    "    print(\"\\n=== Decision Tree Optimization ===\")\n",
    "\n",
    "    # Parameter grid\n",
    "    param_grid = {\n",
    "        'classifier__criterion': ['gini', 'entropy'],\n",
    "        'classifier__max_depth': [None, 5, 10, 15, 20],\n",
    "        'classifier__min_samples_split': [2, 5, 10],\n",
    "        'classifier__min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    # Create base pipelne\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('scaler', StandardScaler(with_mean=False)),\n",
    "        ('classifier', DecisionTreeClassifier(random_state=42))\n",
    "    ])\n",
    "\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='f1_weighted', n_jobs=-1)\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "    best_dt = DecisionTreeClassifier(**{k.replace('classifier__', ''): v\n",
    "                                      for k, v in grid_search.best_params_.items()\n",
    "                                      if k.startswith('classifier__')},\n",
    "                                   random_state=42)\n",
    "\n",
    "    # Visualize tree (optional)\n",
    "    # We need to apply preprocessing first to visualize\n",
    "    X_processed = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('scaler', StandardScaler(with_mean=False))\n",
    "    ]).fit_transform(X)\n",
    "\n",
    "    best_dt.fit(X_processed, y)\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plot_tree(best_dt, filled=True, max_depth=3, feature_names=None)\n",
    "    plt.title(\"Decision Tree (Limited to depth 3 for visualization)\")\n",
    "    plt.show()\n",
    "\n",
    "    # Return both the best model and feature importances\n",
    "    importances = best_dt.feature_importances_\n",
    "\n",
    "    return best_dt, importances\n"
   ],
   "id": "36a699a3b5cc32f2",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T21:07:31.332471Z",
     "start_time": "2025-04-05T21:07:31.190505Z"
    }
   },
   "cell_type": "code",
   "source": " df = load_data('airlines_delay.csv')",
   "id": "6367f13d05bf8e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (539382, 8)\n",
      "\n",
      "Sample data:\n",
      "   Flight    Time  Length Airline AirportFrom AirportTo  DayOfWeek  Class\n",
      "0  2313.0  1296.0   141.0      DL         ATL       HOU          1      0\n",
      "1  6948.0   360.0   146.0      OO         COS       ORD          4      0\n",
      "2  1247.0  1170.0   143.0      B6         BOS       CLT          3      0\n",
      "3    31.0  1410.0   344.0      US         OGG       PHX          6      0\n",
      "4   563.0   692.0    98.0      FL         BMI       ATL          4      0\n",
      "\n",
      "Data types:\n",
      "Flight         float64\n",
      "Time           float64\n",
      "Length         float64\n",
      "Airline         object\n",
      "AirportFrom     object\n",
      "AirportTo       object\n",
      "DayOfWeek        int64\n",
      "Class            int64\n",
      "dtype: object\n",
      "\n",
      "Missing values:\n",
      "Flight         0\n",
      "Time           0\n",
      "Length         0\n",
      "Airline        0\n",
      "AirportFrom    0\n",
      "AirportTo      0\n",
      "DayOfWeek      0\n",
      "Class          0\n",
      "dtype: int64\n",
      "\n",
      "Class distribution:\n",
      "Class\n",
      "0    299118\n",
      "1    240264\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T21:07:31.375743Z",
     "start_time": "2025-04-05T21:07:31.338147Z"
    }
   },
   "cell_type": "code",
   "source": " X, y, preprocessor, numeric_features, categorical_features = preprocess_data(df)",
   "id": "74f7815601f80958",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T21:07:31.384287Z",
     "start_time": "2025-04-05T21:07:31.381924Z"
    }
   },
   "cell_type": "code",
   "source": "best_models = {}",
   "id": "eea99e7e542bf280",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T21:18:17.648731Z",
     "start_time": "2025-04-05T21:07:31.391764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_dt, dt_importances = optimize_decision_tree(X, y, preprocessor)\n",
    "best_models['dt'] = best_dt\n",
    "dt_results = evaluate_model(best_dt, X, y, preprocessor)\n",
    "report_results(\"Decision Tree\", dt_results)"
   ],
   "id": "75a7d134761557ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Decision Tree Optimization ===\n",
      "Best parameters: {'classifier__criterion': 'gini', 'classifier__max_depth': 20, 'classifier__min_samples_leaf': 4, 'classifier__min_samples_split': 10}\n",
      "Best cross-validation score: 0.6403\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot center sparse matrices: pass `with_mean=False` instead. See docstring for motivation and alternatives.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[41]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m best_dt, dt_importances = \u001B[43moptimize_decision_tree\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocessor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      2\u001B[39m best_models[\u001B[33m'\u001B[39m\u001B[33mdt\u001B[39m\u001B[33m'\u001B[39m] = best_dt\n\u001B[32m      3\u001B[39m dt_results = evaluate_model(best_dt, X, y, preprocessor)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[37]\u001B[39m\u001B[32m, line 35\u001B[39m, in \u001B[36moptimize_decision_tree\u001B[39m\u001B[34m(X, y, preprocessor, cv)\u001B[39m\n\u001B[32m     25\u001B[39m best_dt = DecisionTreeClassifier(**{k.replace(\u001B[33m'\u001B[39m\u001B[33mclassifier__\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33m'\u001B[39m): v\n\u001B[32m     26\u001B[39m                                   \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m grid_search.best_params_.items()\n\u001B[32m     27\u001B[39m                                   \u001B[38;5;28;01mif\u001B[39;00m k.startswith(\u001B[33m'\u001B[39m\u001B[33mclassifier__\u001B[39m\u001B[33m'\u001B[39m)},\n\u001B[32m     28\u001B[39m                                random_state=\u001B[32m42\u001B[39m)\n\u001B[32m     30\u001B[39m \u001B[38;5;66;03m# Visualize tree (optional)\u001B[39;00m\n\u001B[32m     31\u001B[39m \u001B[38;5;66;03m# We need to apply preprocessing first to visualize\u001B[39;00m\n\u001B[32m     32\u001B[39m X_processed = \u001B[43mPipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43msteps\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\n\u001B[32m     33\u001B[39m \u001B[43m    \u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mpreprocessor\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocessor\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     34\u001B[39m \u001B[43m    \u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mscaler\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mStandardScaler\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m---> \u001B[39m\u001B[32m35\u001B[39m \u001B[43m\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     37\u001B[39m best_dt.fit(X_processed, y)\n\u001B[32m     39\u001B[39m plt.figure(figsize=(\u001B[32m20\u001B[39m, \u001B[32m10\u001B[39m))\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Repos\\DataMining\\.venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1382\u001B[39m     estimator._validate_params()\n\u001B[32m   1384\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1385\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1386\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1387\u001B[39m     )\n\u001B[32m   1388\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1389\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Repos\\DataMining\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:730\u001B[39m, in \u001B[36mPipeline.fit_transform\u001B[39m\u001B[34m(self, X, y, **params)\u001B[39m\n\u001B[32m    724\u001B[39m last_step_params = \u001B[38;5;28mself\u001B[39m._get_metadata_for_step(\n\u001B[32m    725\u001B[39m     step_idx=\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m) - \u001B[32m1\u001B[39m,\n\u001B[32m    726\u001B[39m     step_params=routed_params[\u001B[38;5;28mself\u001B[39m.steps[-\u001B[32m1\u001B[39m][\u001B[32m0\u001B[39m]],\n\u001B[32m    727\u001B[39m     all_params=params,\n\u001B[32m    728\u001B[39m )\n\u001B[32m    729\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(last_step, \u001B[33m\"\u001B[39m\u001B[33mfit_transform\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m730\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlast_step\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    731\u001B[39m \u001B[43m        \u001B[49m\u001B[43mXt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mlast_step_params\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfit_transform\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[32m    732\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    733\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    734\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m last_step.fit(Xt, y, **last_step_params[\u001B[33m\"\u001B[39m\u001B[33mfit\u001B[39m\u001B[33m\"\u001B[39m]).transform(\n\u001B[32m    735\u001B[39m         Xt, **last_step_params[\u001B[33m\"\u001B[39m\u001B[33mtransform\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    736\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Repos\\DataMining\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001B[39m, in \u001B[36m_wrap_method_output.<locals>.wrapped\u001B[39m\u001B[34m(self, X, *args, **kwargs)\u001B[39m\n\u001B[32m    317\u001B[39m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[32m    318\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, *args, **kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m319\u001B[39m     data_to_wrap = \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    320\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[32m    321\u001B[39m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[32m    322\u001B[39m         return_tuple = (\n\u001B[32m    323\u001B[39m             _wrap_data_with_container(method, data_to_wrap[\u001B[32m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[32m    324\u001B[39m             *data_to_wrap[\u001B[32m1\u001B[39m:],\n\u001B[32m    325\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Repos\\DataMining\\.venv\\Lib\\site-packages\\sklearn\\base.py:918\u001B[39m, in \u001B[36mTransformerMixin.fit_transform\u001B[39m\u001B[34m(self, X, y, **fit_params)\u001B[39m\n\u001B[32m    903\u001B[39m         warnings.warn(\n\u001B[32m    904\u001B[39m             (\n\u001B[32m    905\u001B[39m                 \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mThis object (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m) has a `transform`\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m    913\u001B[39m             \u001B[38;5;167;01mUserWarning\u001B[39;00m,\n\u001B[32m    914\u001B[39m         )\n\u001B[32m    916\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    917\u001B[39m     \u001B[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m918\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m.transform(X)\n\u001B[32m    919\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    920\u001B[39m     \u001B[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001B[39;00m\n\u001B[32m    921\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.fit(X, y, **fit_params).transform(X)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Repos\\DataMining\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:894\u001B[39m, in \u001B[36mStandardScaler.fit\u001B[39m\u001B[34m(self, X, y, sample_weight)\u001B[39m\n\u001B[32m    892\u001B[39m \u001B[38;5;66;03m# Reset internal state before fitting\u001B[39;00m\n\u001B[32m    893\u001B[39m \u001B[38;5;28mself\u001B[39m._reset()\n\u001B[32m--> \u001B[39m\u001B[32m894\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpartial_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Repos\\DataMining\\.venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1382\u001B[39m     estimator._validate_params()\n\u001B[32m   1384\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1385\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1386\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1387\u001B[39m     )\n\u001B[32m   1388\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1389\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Repos\\DataMining\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:959\u001B[39m, in \u001B[36mStandardScaler.partial_fit\u001B[39m\u001B[34m(self, X, y, sample_weight)\u001B[39m\n\u001B[32m    957\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m sparse.issparse(X):\n\u001B[32m    958\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.with_mean:\n\u001B[32m--> \u001B[39m\u001B[32m959\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    960\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mCannot center sparse matrices: pass `with_mean=False` \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    961\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33minstead. See docstring for motivation and alternatives.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    962\u001B[39m         )\n\u001B[32m    963\u001B[39m     sparse_constructor = (\n\u001B[32m    964\u001B[39m         sparse.csr_matrix \u001B[38;5;28;01mif\u001B[39;00m X.format == \u001B[33m\"\u001B[39m\u001B[33mcsr\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m sparse.csc_matrix\n\u001B[32m    965\u001B[39m     )\n\u001B[32m    967\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.with_std:\n\u001B[32m    968\u001B[39m         \u001B[38;5;66;03m# First pass\u001B[39;00m\n",
      "\u001B[31mValueError\u001B[39m: Cannot center sparse matrices: pass `with_mean=False` instead. See docstring for motivation and alternatives."
     ]
    }
   ],
   "execution_count": 41
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
